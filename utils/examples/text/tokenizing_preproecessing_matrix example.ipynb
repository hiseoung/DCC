{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a108538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5b975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../data/뉴스'\n",
    "\n",
    "train_token_path = os.path.join(data_path, 'token.csv')\n",
    "\n",
    "train_token = pd.read_csv(train_token_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4392302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 함수\n",
    "# 정규표현식으로 전처리 함\n",
    "# default로 숫자, 한글, 영어 제외 모두 제거\n",
    "# pandas의 apply 함수와 함께 사용해도 좋을 듯\n",
    "# x : 전처리할 문장\n",
    "# reg : 정규표현식\n",
    "def reg_preprocessing(x, reg=r'[^\\d가-힣a-zA-Z ]'):\n",
    "    x = re.sub(reg, '', x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# 문장을 토큰화 한 후 index로 변경\n",
    "# 문장을 sequence로 변경\n",
    "# text : 토큰화 할 text(train set)\n",
    "# max_len : 훈련 시킬 문장의 최대 길이\n",
    "# oov : 처음 보는 단어에 대한 token. fasttext는 필요 x\n",
    "# num_words : 사용할 단어의 최소 빈도 수. model fit 할 때의 수랑 맞춰야 함\n",
    "# pad_option : padding 및 trunc 때 앞에서 부터 할지 뒤에서 부터 할지\n",
    "# 필요 module\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def make_token_and_tokenizer(text, max_len=100, oov=None, num_words=None, pad_option='post'):\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    token_index = tokenizer.texts_to_sequences(text)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    token_index = pad_sequences(token_index, maxlen=max_len, padding=pad_option, truncating=pad_option)\n",
    "\n",
    "    return token_index, vocab_size, tokenizer\n",
    "\n",
    "\n",
    "# tf Embedding layer에 weight로 사용할 matrix를 만드는 함수\n",
    "# word_dict : 단어와 인덱스 번호. tokenizer.word_index. 0은 padding\n",
    "# model : embedding 모델(fasttext를 생각하고 만듦)\n",
    "# vocab_size : make_token_and_tokenizer에서 얻어지는 vocab_size.\n",
    "# embedding_size : 임베딩 차원. fasttext는 100이 default\n",
    "# 필요 module\n",
    "# 없음. 모델만 잘 넣어주면 됨\n",
    "def make_embedding_matrix(word_dict, model, vocab_size, embedding_size=100):\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "    for word, index in word_dict.items():\n",
    "        embedding_matrix[index] = model.wv[word]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75828e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   인천  핀란드 항공기 결항 휴가 철 여행객 분통\n",
       "1              실리콘밸리 넘어서겠다 구글 15조원 들여  전역 거점 화\n",
       "2              이란 외무 긴장 완화 해결 책 미국 경제 전쟁 멈추는 것\n",
       "3         NYT 클린턴 측근  기업 특수 관계 조명 공과 사 맞 물려 종합\n",
       "4                    시진핑 트럼프 중미 무역 협상 조속 타결 희망\n",
       "                         ...                  \n",
       "45649             KB 금융 미국 IB 스티펠 제휴 선진국 시장 공략\n",
       "45650        1 보 서울시 교육청 신종 코로나 확산 개학 연기 휴업 검토\n",
       "45651          게시판 키움 증권 2020 키움 영웅 전 실전 투자 대회\n",
       "45652                   답변 하는 배 기동 국립 중앙 박물관 장\n",
       "45653    2020 한국 인터넷 기자 상 시상식 내달 1일 개최 특별상 김성후\n",
       "Name: title, Length: 45654, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply를 통한 전처리\n",
    "train_token['title'] = train_token['title'].apply(reg_preprocessing)\n",
    "train_token['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d05798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트로 변환\n",
    "tokenize_data = []\n",
    "for sentence in train_token['title'].tolist():\n",
    "    tokenize_data.append(sentence.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8a0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext fit\n",
    "model = FastText(tokenize_data, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6548d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 생성\n",
    "token_index, vocab_size, tokenizer = make_token_and_tokenizer(train_token['title'].tolist(), num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaabc52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5281601 ,  0.21827047, -0.09583736, -0.05389892,  0.05651149,\n",
       "       -0.32731995, -0.30143198,  0.31808397,  0.07005626,  0.06872333,\n",
       "        0.27182803, -0.31650779,  0.03053061,  0.0317923 , -0.18763359,\n",
       "       -0.10731325,  0.21604368, -0.35591209,  0.14379723, -0.07971345,\n",
       "        0.31946507, -0.40950027, -0.12266451, -0.32632136, -0.12529513,\n",
       "        0.22423929,  0.1129882 ,  0.11344469, -0.34932217, -0.38271603,\n",
       "       -0.17162028, -0.1896355 ,  0.21483681, -0.12851153, -0.00591639,\n",
       "       -0.08763758,  0.26277113, -0.3409645 , -0.02513706, -0.32403067,\n",
       "       -0.00354818, -0.03306084, -0.33180583,  0.3900927 ,  0.10460509,\n",
       "        0.18834165,  0.36209768, -0.05974977,  0.02914863,  0.00660153,\n",
       "        0.15025064, -0.15514784, -0.30007142, -0.14558642,  0.03472638,\n",
       "       -0.11497443,  0.47033048,  0.01644604, -0.2183286 ,  0.29826608,\n",
       "        0.05926862, -0.02271334,  0.23134229,  0.2029067 ,  0.02538442,\n",
       "        0.20853975,  0.73334974, -0.13216019,  0.00694963,  0.11628205,\n",
       "        0.08981997,  0.24686366,  0.07960006, -0.16014832,  0.23321229,\n",
       "       -0.33168107, -0.22527218,  0.16577251, -0.42885375, -0.51967192,\n",
       "       -0.52454555, -0.12415108, -0.07127783, -0.07790588,  0.15071522,\n",
       "       -0.02004436,  0.07572625, -0.40147871,  0.60439467, -0.26682553,\n",
       "        0.19690296, -0.26025623,  0.39735204, -0.27649486,  0.40787947,\n",
       "        0.46729055,  0.04775745, -0.2093043 ,  0.04586411,  0.38848096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding matrix 생성\n",
    "embedding_matrix = make_embedding_matrix(tokenizer.word_index, model, vocab_size)\n",
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f9dd21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 종합 index는 1\n",
    "tokenizer.word_index['종합']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d39e2f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5281601 ,  0.21827047, -0.09583736, -0.05389892,  0.05651149,\n",
       "       -0.32731995, -0.30143198,  0.31808397,  0.07005626,  0.06872333,\n",
       "        0.27182803, -0.3165078 ,  0.03053061,  0.0317923 , -0.18763359,\n",
       "       -0.10731325,  0.21604368, -0.3559121 ,  0.14379723, -0.07971345,\n",
       "        0.31946507, -0.40950027, -0.12266451, -0.32632136, -0.12529513,\n",
       "        0.22423929,  0.1129882 ,  0.11344469, -0.34932217, -0.38271603,\n",
       "       -0.17162028, -0.1896355 ,  0.2148368 , -0.12851153, -0.00591639,\n",
       "       -0.08763758,  0.26277113, -0.3409645 , -0.02513706, -0.32403067,\n",
       "       -0.00354818, -0.03306084, -0.33180583,  0.3900927 ,  0.10460509,\n",
       "        0.18834165,  0.36209768, -0.05974977,  0.02914863,  0.00660153,\n",
       "        0.15025064, -0.15514784, -0.30007142, -0.14558642,  0.03472638,\n",
       "       -0.11497443,  0.47033048,  0.01644604, -0.2183286 ,  0.29826608,\n",
       "        0.05926862, -0.02271334,  0.23134229,  0.2029067 ,  0.02538442,\n",
       "        0.20853975,  0.73334974, -0.13216019,  0.00694963,  0.11628205,\n",
       "        0.08981997,  0.24686366,  0.07960006, -0.16014832,  0.23321229,\n",
       "       -0.33168107, -0.22527218,  0.16577251, -0.42885375, -0.5196719 ,\n",
       "       -0.52454555, -0.12415108, -0.07127783, -0.07790588,  0.15071522,\n",
       "       -0.02004436,  0.07572625, -0.4014787 ,  0.6043947 , -0.26682553,\n",
       "        0.19690296, -0.26025623,  0.39735204, -0.27649486,  0.40787947,\n",
       "        0.46729055,  0.04775745, -0.2093043 ,  0.04586411,  0.38848096],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding matrix와 동일\n",
    "model.wv['종합']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f8dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b948fd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([-0.5281601 ,  0.21827047, -0.09583736, -0.05389892,  0.05651149,\n",
       "       -0.32731995, -0.30143198,  0.31808397,  0.07005626,  0.06872333,\n",
       "        0.27182803, -0.3165078 ,  0.03053061,  0.0317923 , -0.18763359,\n",
       "       -0.10731325,  0.21604368, -0.3559121 ,  0.14379723, -0.07971345,\n",
       "        0.31946507, -0.40950027, -0.12266451, -0.32632136, -0.12529513,\n",
       "        0.22423929,  0.1129882 ,  0.11344469, -0.34932217, -0.38271603,\n",
       "       -0.17162028, -0.1896355 ,  0.2148368 , -0.12851153, -0.00591639,\n",
       "       -0.08763758,  0.26277113, -0.3409645 , -0.02513706, -0.32403067,\n",
       "       -0.00354818, -0.03306084, -0.33180583,  0.3900927 ,  0.10460509,\n",
       "        0.18834165,  0.36209768, -0.05974977,  0.02914863,  0.00660153,\n",
       "        0.15025064, -0.15514784, -0.30007142, -0.14558642,  0.03472638,\n",
       "       -0.11497443,  0.47033048,  0.01644604, -0.2183286 ,  0.29826608,\n",
       "        0.05926862, -0.02271334,  0.23134229,  0.2029067 ,  0.02538442,\n",
       "        0.20853975,  0.73334974, -0.13216019,  0.00694963,  0.11628205,\n",
       "        0.08981997,  0.24686366,  0.07960006, -0.16014832,  0.23321229,\n",
       "       -0.33168107, -0.22527218,  0.16577251, -0.42885375, -0.5196719 ,\n",
       "       -0.52454555, -0.12415108, -0.07127783, -0.07790588,  0.15071522,\n",
       "       -0.02004436,  0.07572625, -0.4014787 ,  0.6043947 , -0.26682553,\n",
       "        0.19690296, -0.26025623,  0.39735204, -0.27649486,  0.40787947,\n",
       "        0.46729055,  0.04775745, -0.2093043 ,  0.04586411,  0.38848096],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding matrix, model의 종합 벡터와 동일\n",
    "embed_layer(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c183aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCC",
   "language": "python",
   "name": "dcc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
